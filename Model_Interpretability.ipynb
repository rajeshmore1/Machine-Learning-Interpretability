{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2hTC5f0IqPzyTKChCFtcE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajeshmore1/Machine-Learning-Interpretability/blob/main/Model_Interpretability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretability\n",
        "\n",
        "* Interpretability is the degree to which a human can understand the cause of\n",
        "a decision. \n",
        "\n",
        "* The higher the interpretability of a machine learning model, the\n",
        "easier it is for someone to comprehend why certain decisions or predictions have been made.\n",
        "\n",
        "* A model is better interpretable than another model if its decisions are easier for a human to\n",
        "comprehend than decisions from the other model."
      ],
      "metadata": {
        "id": "ki4Z-NKqgo4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importance Of Interpretability\n",
        "\n",
        "* Machine learning models can only be debugged and audited when they can be interpreted\n",
        "\n",
        "* Explanations are used to manage social interactions. For\n",
        "a machine to interact with us, it may need to shape our emotions and beliefs. Machines have to\n",
        "“persuade” us, so that they can achieve their intended goal. \n",
        "\n",
        "We would not fully accept my robot\n",
        "vacuum cleaner if it did not explain its behavior to some degree. The vacuum cleaner creates a\n",
        "shared meaning of, for example, an “accident” (like getting stuck on the bathroom carpet … again) by explaining that it got stuck instead of simply stopping to work without comment. Interestingly,\n",
        "there may be a misalignment between the goal of the explaining machine (create trust) and the goal\n",
        "of the recipient (understand the prediction or behavior).\n",
        "\n",
        "* The process of integrating machines and algorithms into our daily lives requires interpretability to\n",
        "increase social acceptance."
      ],
      "metadata": {
        "id": "2wAXbzJMliYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Of Machine Learning Interpretability\n",
        "\n",
        "## 1. Intrinsic Vs Post hoc\n",
        "\n",
        "* This criteria distinguishes whether interpretability is achieved by restricting\n",
        "the complexity of the machine learning model (intrinsic) or by applying methods that analyze the\n",
        "model after training (post hoc).\n",
        "\n",
        "* **Intrinsic interpretability** refers to machine learning models that are\n",
        "considered interpretable due to their simple structure, such as short decision trees or sparse linear\n",
        "models.\n",
        "\n",
        "* **Post hoc interpretability** refers to the application of interpretation methods after model\n",
        "training. Permutation feature importance is, for example, a post hoc interpretation method. Post\n",
        "hoc methods can also be applied to intrinsically interpretable models. For example, permutation\n",
        "feature importance can be computed for decision trees.\n",
        "\n",
        "### Result of the interpretation method:\n",
        " The various interpretation methods can be roughly differentiated according to their results.\n",
        "\n",
        "* **Feature summary statistic:** Many interpretation methods provide **summary statistics** for each\n",
        "feature. Some methods return a single number per feature, such as **feature importance,** or a\n",
        "more complex result, such as the pairwise feature interaction strengths, which consist of a\n",
        "number for each feature pair.\n",
        "\n",
        "*  **Feature summary visualization:** Visualisation with the hep of **partial dependence graph**.Partial dependence plots\n",
        "are curves that show a feature and the average predicted outcome. \n",
        "\n",
        "* **Model internals (e.g. learned weights):** The interpretation of intrinsically interpretable\n",
        "models falls into this category. Examples are the weights in linear models or the learned tree\n",
        "structure (the features and thresholds used for the splits) of decision trees.\n",
        "\n",
        "* **Data point:** This category includes all methods that return data points (already existent or newly created) to make a model interpretable. One method is called **counterfactual explanations.** To explain the prediction of a data instance, the method finds a similar data point by\n",
        "changing some of the features for which the predicted outcome changes in a relevant way (e.g. a flip in the predicted class).\n",
        "\n"
      ],
      "metadata": {
        "id": "KmQVOOeCrW23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model-specific or model-agnostic? \n",
        "\n",
        " **Model-specific interpretation tools** are limited to specific model\n",
        "classes. \n",
        "\n",
        "* e.g The interpretation of regression weights in a linear model is a model-specific interpretation.\n",
        "\n",
        "* The interpretation of intrinsically interpretable models is always modelspecific. \n",
        "\n",
        "**Model-agnostic tools** can be used on any machine learning model and are applied after the model\n",
        "has been trained **(post hoc)**. \n",
        "\n",
        "* These agnostic methods usually work by analyzing feature input and output pairs. \n",
        "\n",
        "* These methods cannot have access to model internals such as weights."
      ],
      "metadata": {
        "id": "SI0G-BHxQ2RT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w9EQAhqbgsNw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}